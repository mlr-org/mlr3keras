---
title: "first_steps"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{entity_embeddings}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Using mlr3keras

This vignette is used to showcase different aspects of `mlr3keras`.
As `mlr3keras` is still under heavy development, this place might be a good way
to look for currently working aspects of the package.

For other aspects, we aim to sketch proposed functionality in order to enable discussion.

```{r, message=FALSE}
library("mlr3")
library("mlr3keras")
```

## mlr3keras in 5 lines

```{r, message=FALSE}
# Instantiate Learner
lrn = LearnerClassifKerasFF$new()

# Set hyperparameters
lrn$param_set$values$epochs = 50
lrn$param_set$values$layer_units = c(12, 30)

# Train and predict
lrn$train(mlr_tasks$get("iris"))
lrn$predict(mlr_tasks$get("iris"))
```

## A simple first example

This first simple example showcases how to use `mlr3keras` in its simplest form.
We use it together with `mlr3pipelines` in order to fit a model on a dataset,
in this case `pima` with missing values.

Before we fit the model, we thus impute every missing variable with its mean.

Before we build up the pipeline, we define and `compile` the model we are going to use.
This follows the `keras` API, see for example the [RStudio Keras Documentation](https://keras.rstudio.com/) for details.

```{r}
library("keras")
model = keras_model_sequential() %>%
layer_dense(units = 12L, input_shape = 8L, activation = "relu") %>%
layer_dense(units = 12L, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid") %>%
  compile(optimizer = optimizer_adam(3*10^-4),
    loss = "binary_crossentropy",
    metrics = "accuracy")
```

Afterwards, we build up the pipeline using the `classif.keras` learner.
We set the `model` defined above as a hyperparameter, as well as the
number of `epochs` we want to train our model for.

```{r,message=FALSE}
library(mlr3pipelines)
po_imp = PipeOpImputeMedian$new()
po_lrn = PipeOpLearner$new(lrn("classif.keras"))
po_lrn$param_set$values$model = model
po_lrn$param_set$values$epochs = 10L
pipe = po_imp %>>% po_lrn
```

We now have a finished `pipe`, a `Pipeline` which can be used, either as a `Learner` in
conjunction with `GraphLearner` or simply to train and predict.

```{r,message=FALSE}
pipe$train(mlr_tasks$get("pima"))
```

The trained model gives us access to different methods for further inspection:

```{r}
pipe$pipeops$classif.keras
```

## Tuning over model architectures

Consider the following scenario:
We define multiple `architectures`, and now we aim to find out which
architecture works best.
`mlr3tuning` does not directly allow to tune over architectures, and we have to use a little
trick for now:

Assume our architectures can be obtained from a function that looks as follows:

```{r}
get_keras_model = function(arch = "arch1", lr = 3*10^-4) {
  if (arch == "arch1") {
    model = keras_model_sequential() %>%
      layer_dense(units = 16L, input_shape = 10L, activation = "relu") %>%
      layer_dense(units = 16L, activation = "relu") %>%
      layer_dense(units = 1L, activation = "linear")
  } else if (arch == "arch2") {
    model = keras_model_sequential() %>%
      layer_dense(units = 64L, input_shape = 10L, activation = "relu") %>%
      layer_dense(units = 32L, activation = "relu") %>%
      layer_dense(units = 1L, activation = "linear")
  }
  model %>%
    compile(optimizer = optimizer_adam(lr),
      loss = "binary_crossentropy",
      metrics = "accuracy")
}
```

And now we aim to select the better architecture between `arch1` and `arch2`, and
additionally a learning rate **lr**.

First we create a `ParamSet` we want to tune over:
In order to work with arbitrary types, we have to use a little trick:
Instead of using `ParamUty`, we create a `ParamFct` with a fixed set of levels.
Afterwards we use a `trafo` in order to transform the `ParamFct's` levels
to a keras model.

In the `trafo` we have to do two things:
* Create a keras model from our arguments (i.e. `arch` and `lr`)
* Delete the original `arch` and `lr` Parameters, as the *keras* learner does not know
  what to do with them.

```{r}
library("paradox")
ps = ParamSet$new(list(
  ParamFct$new("arch", levels = c("arch1", "arch2"), tags = "train"),
  ParamDbl$new("lr", lower = 10^-5, upper = 10^-2, tags = "train")
))
ps$trafo = function(x, param_set) {
      x$model = get_keras_model(x$arch, x$lr)
      x$lr = x$arch = NULL
      return(x)
}
```

Now we can tune over the architecture space like we are used to from **mlr3tuning**.

```{r}
library("mlr3tuning")
learner = lrn("regr.keras", callbacks = list(cb_es(3)))
task = mlr_tasks$get("mtcars")
resampling = rsmp("holdout")
measure = msr("regr.mse")
tuner = tnr("grid_search", resolution = 2)
terminator = term("evals", n_evals = 4)
instance = TuningInstance$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measures = measure,
  param_set = ps,
  terminator = terminator
)
tuner$tune(instance)
```

## Entity Embeddings

Entity embeddings are naturally embedded into the `kerasff` learners.

## Custom Optimizers & Activation Functions

In the example below, we will use the `radam` (rectified adam) optimizer
in order to train our neural network.
We will first have to install the python package that contains the
optimizer using `pip` or `conda`.

Afterwards we can check whether the package was successfully installed
via:
```{r, eval = FALSE}
library("reticulate")
py_module_available("keras_radam")
```

Now we can import the optimizer and use it to train our learner.

```{r, eval = FALSE}
kr = import("keras_radam")
radam = kr$training$RAdamOptimizer()
lrn = lrn("classif.kerasff", predict_type = "prob", epochs = 3L, optimizer = radam)
lrn$train(mlr_tasks$get("iris"))
```