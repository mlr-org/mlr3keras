% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/keras_callbacks.R
\name{cb_es}
\alias{cb_es}
\alias{cb_lr_scheduler_cosine_anneal}
\alias{cb_lr_scheduler_exponential_decay}
\alias{cb_tensorboard}
\alias{cb_lr_log}
\alias{LogMetrics}
\alias{SetLogLR}
\title{\code{cb_es}: Early stopping callback}
\usage{
cb_es(monitor = "val_loss", patience = 3L)

cb_lr_scheduler_cosine_anneal(
  eta_max = 0.01,
  T_max = 10,
  T_mult = 2,
  M_mult = 1,
  eta_min = 0
)

cb_lr_scheduler_exponential_decay()

cb_tensorboard()

cb_lr_log()
}
\arguments{
\item{monitor}{\code{\link{character}}\cr
Quantity to be monitored.}

\item{patience}{\code{\link{integer}}\cr
Number of iterations without improvement to wait before stopping.}

\item{eta_max}{\code{\link{numeric}}\cr
Max  learning rate.}

\item{T_max}{\code{\link{integer}}\cr
Reset learning rate every T_max epochs.  Default 10.}

\item{T_mult}{\code{\link{integer}}\cr
Multiply T_max by T_mult every T_max iterations. Default 2.}

\item{M_mult}{\code{\link{numeric}}\cr
Decay learning rate by factor 'M_mult' after each learning rate reset.}

\item{eta_min}{\code{\link{numeric}}\cr
Minimal learning rate.}
}
\description{
For more information see:
Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983.
}
\details{
Closed form:
\eqn{\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 +
         \cos(\frac{T_{cur}}{T_{max}}\pi))}
}
