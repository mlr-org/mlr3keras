% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LearnerKerasCNN.R
\name{LearnerClassifKerasCNN}
\alias{LearnerClassifKerasCNN}
\alias{mlr_learners_classif.kerascnn}
\title{Keras CNN Architectures for Classification}
\format{
\code{\link[R6:R6Class]{R6::R6Class()}} inheriting from \link{LearnerClassifKeras}.
}
\description{
Convolutional Neural Network (CNN) application from \CRANpkg{keras}.
This learner builds and compiles the keras model from the hyperparameters in \code{param_set},
and does not require a supplied and compiled model.
The 'application' parameter refers to a 'keras::application_*' CNN architectures,
possibly with pre-trained weights.

Calls \link[keras:fit_generator]{keras::fit_generator} together with \link[keras:flow_images_from_dataframe]{keras::flow_images_from_dataframe}  from package \CRANpkg{keras}.
Layers are set up as follows:
\itemize{
\item The last layer (classification layer) is cut off the neural network.
\item A classification layer with 'cl_layer_units' is added.
\item The weights of all layers are frozen.
\item The last 'unfreeze_n_last_layers' are unfrozen.
}

Parameters:\cr
Most of the parameters can be obtained from the \code{keras} documentation.
Some exceptions are documented here.
\itemize{
\item \code{application}: A (possibly pre-trained) CNN architecture.
Default: \link[keras:application_resnet50]{keras::application_resnet50}.
\item \code{cl_layer_units}: Number of units in the classification layer.
\item \code{unfreeze_n_last_layers}: Number of last layers to be unfrozen.
\item \code{optimizer}: Some optimizers and their arguments can be found below.\cr
Inherits from \code{tensorflow.python.keras.optimizer_v2}.\preformatted{"sgd"     : optimizer_sgd(lr, momentum, decay = decay),
"rmsprop" : optimizer_rmsprop(lr, rho, decay = decay),
"adagrad" : optimizer_adagrad(lr, decay = decay),
"adam"    : optimizer_adam(lr, beta_1, beta_2, decay = decay),
"nadam"   : optimizer_nadam(lr, beta_1, beta_2, schedule_decay = decay)
}
\item \code{class_weights}: needs to be a named list of class-weights
for the different classes numbered from 0 to c-1 (for c classes).\preformatted{Example:
wts = c(0.5, 1)
setNames(as.list(wts), seq_len(length(wts)) - 1)
}
\item \code{callbacks}: A list of keras callbacks.
See \code{?callbacks}.
}
}
\section{Construction}{
\preformatted{LearnerClassifKerasCNN$new()
mlr3::mlr_learners$get("classif.kerascnn")
mlr3::lrn("classif.kerascnn")
}
}

\section{Learner Methods}{


Keras Learners offer several methods for easy access to the
stored models.
\itemize{
\item \code{.$plot()}\cr
Plots the history, i.e. the train-validation loss during training.
\item \code{.$save(file_path)}\cr
Dumps the model to a provided file_path in 'h5' format.
\item \code{.$load_model_from_file(file_path)}\cr
Loads a model saved using \code{saved} back into the learner.
The model needs to be saved separately when the learner is serialized.
In this case, the learner can be restored from this function.
Currently not implemented for 'TabNet'.
\item \code{.$lr_find(task, epochs, lr_min, lr_max, batch_size)}\cr
Employ an implementation of the learning rate finder as popularized by
Jeremy Howard in fast.ai (http://course.fast.ai/) for the learner.
For more info on parameters, see \code{find_lr}.
}
}

\examples{
learner = mlr3::lrn("classif.kerascnn")
print(learner)

# available parameters:
learner$param_set$ids()
}
\seealso{
\link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}
}
