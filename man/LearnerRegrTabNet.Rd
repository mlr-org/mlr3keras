% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LearnerTabNet.R
\name{LearnerRegrTabNet}
\alias{LearnerRegrTabNet}
\alias{mlr_learners_regr.tabnet}
\title{Keras TabNet Neural Network for Regression}
\format{
\code{\link[R6:R6Class]{R6::R6Class()}} inheriting from \link{LearnerRegrKeras}.
}
\description{
Implementation of "TabNet" from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details.
}
\section{Construction}{
\preformatted{LearnerRegrTabNet$new()
mlr3::mlr_learners$get("regr.tabnet")
mlr3::lrn("regr.tabnet")
}
}

\section{Hyper Parameter Tuning}{


Additional Arguments:
\itemize{
\item \code{embed_size}: Size of embedding for categorical, character and ordered factors.
Defaults to \code{min(600L, round(1.6 * length(levels)^0.56))}.
\item \code{stacked}: Should a \code{StackedTabNetModel} be used instead of a normal \code{TabNetModel}? \cr
}
}

\section{Excerpt from paper}{

We consider datasets ranging from 10K to 10M training points, with varying degrees of fitting
difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter
selection: \cr
\itemize{
\item Most datasets yield the best results for Nsteps between 3 and 10. Typically, larger datasets and
more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from
overfitting and yield poor generalization. \cr
\item Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off
between performance and complexity. Nd = Na is a reasonable choice for most datasets. A
very high value of Nd and Na may suffer from overfitting and yield poor generalization. \cr
\item An optimal choice of \eqn{\gamma} can have a major role on the overall performance. Typically a larger
Nsteps value favors for a larger \eqn{\gamma}. \cr
\item A large batch size is beneficial for performance - if the memory constraints permit, as large
as 1-10 \% of the total training dataset size is suggested. The virtual batch size is typically
much smaller than the batch size. \cr
\item Initially large learning rate is important, which should be gradually decayed until convergence. \cr
}

The R class wraps a python implementation found in \url{https://github.com/titu1994/tf-TabNet/tree/master/tabnet}.
}

\section{Parameters}{

\itemize{
\item \code{feature_dim} (N_a): Dimensionality of the hidden representation in feature
transformation block. Each layer first maps the representation to a
2*feature_dim-dimensional output and half of it is used to determine the
nonlinearity of the GLU activation where the other half is used as an
input to GLU, and eventually feature_dim-dimensional output is
transferred to the next layer. \cr
\item \code{output_dim} (N_d): Dimensionality of the outputs of each decision step, which is
later mapped to the final classification or regression output. \cr
\item \code{num_features}: The number of input features (i.e the number of columns for
tabular data assuming each feature is represented with 1 dimension). \cr
\item \code{num_decision_steps} (N_steps): Number of sequential decision steps. \cr
\item \code{relaxation_factor} (gamma): Relaxation factor that promotes the reuse of each
feature at different decision steps. When it is 1, a feature is enforced
to be used only at one decision step and as it increases, more \cr
flexibility is provided to use a feature at multiple decision steps.
\item \code{sparsity_coefficient} (lambda_sparse): Strength of the sparsity regularization.
Sparsity may provide a favorable inductive bias for convergence to
higher accuracy for some datasets where most of the input features are redundant. \cr
\item \code{norm_type}: Type of normalization to perform for the model. Can be either
'batch' or 'group'. 'group' is the default. \cr
\item \code{batch_momentum}: Momentum in ghost batch normalization. \cr
\item \code{virtual_batch_size}: Virtual batch size in ghost batch normalization. The
overall batch size should be an integer multiple of virtual_batch_size. \cr
\item \code{num_groups}: Number of groups used for group normalization. The number of groups
should be a divisor of the number of input features (\code{num_features}) \cr
\item \code{epsilon}: A small number for numerical stability of the entropy calculations. \cr
\item \code{num_layers}: Required for stacked tabnet. Automatically set to \code{1L} if not provided. \cr
}
}

\section{Learner Methods}{


Keras Learners offer several methods for easy access to the
stored models.
\itemize{
\item \code{.$plot()}\cr
Plots the history, i.e. the train-validation loss during training.
\item \code{.$save(file_path)}\cr
Dumps the model to a provided file_path in 'h5' format.
\item \code{.$load_model_from_file(file_path)}\cr
Loads a model saved using \code{saved} back into the learner.
The model needs to be saved separately when the learner is serialized.
In this case, the learner can be restored from this function.
Currently not implemented for 'TabNet'.
\item \code{.$lr_find(task, epochs, lr_min, lr_max, batch_size)}\cr
Employ an implementation of the learning rate finder as popularized by
Jeremy Howard in fast.ai (http://course.fast.ai/) for the learner.
For more info on parameters, see \code{find_lr}.
}
}

\examples{
learner = mlr3::lrn("regr.tabnet")
print(learner)

# available parameters:
learner$param_set$ids()
}
\references{
Sercan, A. and Pfister, T. (2019): TabNet. \url{https://arxiv.org/abs/1908.07442}.
}
\seealso{
\link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}
}
