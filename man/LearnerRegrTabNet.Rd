% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LearnerTabNet.R
\name{LearnerRegrTabNet}
\alias{LearnerRegrTabNet}
\alias{mlr_learners_regr.tabnet}
\title{Keras TabNet Neural Network for Regression

Implementation of "TabNet" from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details.}
\format{\code{\link[R6:R6Class]{R6::R6Class()}} inheriting from \link[mlr3keras:LearnerRegrKeras]{mlr3keras::LearnerRegrKeras}.}
\description{
Keras TabNet Neural Network for Regression

Implementation of "TabNet" from the paper TabNet: Attentive Interpretable Tabular Learning (Sercan, Pfister, 2019).
See https://arxiv.org/abs/1908.07442 for details.
}
\section{Construction}{
\preformatted{LearnerRegrTabNet$new()
mlr3::mlr_learners$get("regr.tabnet")
mlr3::lrn("regr.tabnet")
}
}

\section{Hyper Parameter Tuning}{


Additional Arguments:
\itemize{
\item embed_size: Size of embedding for categorical, character and ordered factors.
Defaults to \code{min(600L, round(1.6 * length(levels)^0.56))}.
\item stacked: Should a \code{StackedTabNetModel} be used instead of a normal \code{TabNetModel}?
}

The python implementation can be found in \url{https://github.com/titu1994/tf-TabNet/tree/master/tabnet}.
(Excerpt from paper)
We consider datasets ranging from 10K to 10M training points, with varying degrees of fitting
difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter
selection:
* Most datasets yield the best results for Nsteps between 3 and 10. Typically, larger datasets and
more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from
overfitting and yield poor generalization.
* Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off
between performance and complexity. Nd = Na is a reasonable choice for most datasets. A
very high value of Nd and Na may suffer from overfitting and yield poor generalization.
* An optimal choice of \eqn{\gamma} can have a major role on the overall performance. Typically a larger
Nsteps value favors for a larger \eqn{\gamma}.
* A large batch size is beneficial for performance - if the memory constraints permit, as large
as 1-10 \% of the total training dataset size is suggested. The virtual batch size is typically
much smaller than the batch size.
* Initially large learning rate is important, which should be gradually decayed until convergence.
Parameters:
* feature_dim (N_a): Dimensionality of the hidden representation in feature
transformation block. Each layer first maps the representation to a
2*feature_dim-dimensional output and half of it is used to determine the
nonlinearity of the GLU activation where the other half is used as an
input to GLU, and eventually feature_dim-dimensional output is
transferred to the next layer.
* output_dim (N_d): Dimensionality of the outputs of each decision step, which is
later mapped to the final classification or regression output.
* num_features: The number of input features (i.e the number of columns for
tabular data assuming each feature is represented with 1 dimension).
* num_decision_steps(N_steps): Number of sequential decision steps.
* relaxation_factor (gamma): Relaxation factor that promotes the reuse of each
feature at different decision steps. When it is 1, a feature is enforced
to be used only at one decision step and as it increases, more
flexibility is provided to use a feature at multiple decision steps.
* sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.
Sparsity may provide a favorable inductive bias for convergence to
higher accuracy for some datasets where most of the input features are redundant.
* norm_type: Type of normalization to perform for the model. Can be either
'batch' or 'group'. 'group' is the default.
* batch_momentum: Momentum in ghost batch normalization.
* virtual_batch_size: Virtual batch size in ghost batch normalization. The
overall batch size should be an integer multiple of virtual_batch_size.
* num_groups: Number of groups used for group normalization.
* epsilon: A small number for numerical stability of the entropy calculations.
}

\section{Learner Methods}{


Keras Learners offer several methods for easy access to the
stored models.
\itemize{
\item \code{.$plot()}\cr
Plots the history, i.e. the train-validation loss during training.
\item \code{.$save(file_path)}\cr
Dumps the model to a provided file_path in 'h5' format.
}
}

\examples{
learner = mlr3::lrn("regr.tabnet")
print(learner)

# available parameters:
learner$param_set$ids()
}
\references{
Sercan, A. and Pfister, T. (2019): TabNet. \url{https://arxiv.org/abs/1908.07442}.
}
\seealso{
\link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}
}
